{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torisimon2/Google-Colab/blob/main/pgd_attack_adv_training_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook serves as a brief introduction to adversarial attacks, and implements the Projected Gradient Descent attack as an example. The notebook also introduces an adversarial training defense.\n",
        "\n",
        " Adversarial attacks occur when someone alters clean data in a way that severely worsens a neural network's performance. The altered data are called 'adversarial examples'. The changes to the clean data are usually chosen algorithmically such that they maximize the severity of misclassifications, while minimizing the change to the inputs. This keeps the attack effective yet hard to detect.\n",
        "\n",
        " Adversarial training is a proactive defense method where the defender includes adversarial examples in the model's training data. By seeing 'bad' examples in advance, the model is able to learn to correctly classify adversarial examples, making the model more robust against future attacks."
      ],
      "metadata": {
        "id": "06Pf_V4qcNae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Import the necessary data science and machine learning packages, which will help us throughout this notebook:*"
      ],
      "metadata": {
        "id": "ThujIIY74tss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "udQuWt9kdudD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine that you want to train a neural network model on nuclear reactor sensor data.\n",
        "Each datapoint or row in the spreadsheet represents the readings from 27 sensors at a given timestamp. For each row of sensor readings, there is a label that tells you what the state of the reactor was.\n",
        "\n",
        "Through training on the labeled data, the model will learn mathematical relationships that explain the relationship between the sensor data and the labels. When you encounter unlabeled sensor data in the future, this model should be able to accurately predict labels from the new data, assuming the training data generalizes well to real-world data.\n",
        "\n",
        "*Read the dataset into a pandas dataframe, and display the first 10 rows:*"
      ],
      "metadata": {
        "id": "T4OdbQuIead8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/DunyaBahar/CyberNuke/main/data.csv\")\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "xBXfLxmRdorX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this dataset, there are 12 possible labels. These labels can be found in the 'TRANSIENT' column towards the right side of the dataframe. The 12 categorical labels have already been converted into computer-readable numerical values, but each number 0 - 11 represent the following reactor states:\n",
        "\n",
        "    0: TRANSIENT-Normal Ops\n",
        "    1: Transient-Feedwater Pump Trip\n",
        "    2: Transient-LOCA LOOP\n",
        "    3: Transient Valve Closure\n",
        "    4: Transient Rapid Power Change\n",
        "    5: Transient- Depressurization\n",
        "    6: Transient- Max Steam Line Rupture\n",
        "    7: Transient-Manual Trip\n",
        "    8: Transient Load Rejection\n",
        "    9: Transient Single Coolant Pump Trip\n",
        "    10: Transient Total Coolant Pump Trip\n",
        "    11: Transient Turbine Trip No SCRAM\n",
        "\n",
        "A label of 0 in the TRANSIENT column means normal operations, or steady state conditions. Labels 1 through 11 indicate transient behavior, each with a given cause or explanation.\n",
        "\n",
        "*Define the functions to make and train a neural network classification model for this data:*"
      ],
      "metadata": {
        "id": "3AQhneJ6mlHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function defines the architecture of the neural network.\n",
        "def make_model():\n",
        "  keras.backend.clear_session()\n",
        "  # this defines the model as a sequential model with 3 layers.\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"selu\", input_shape = (27,)),  # the input layer, with 100 neurons and the selu activation function\n",
        "    keras.layers.Dense(64, activation='selu'),  # one hidden layer\n",
        "    keras.layers.Dense(12, activation='softmax')  # the output layer, with 12 neurons (one for each label) and the softmax activation function\n",
        "  ])\n",
        "  # compile the model with specific loss function, optimizer and metric.\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "\n",
        "# this function trains the model on the training data.\n",
        "def train(model, training_data, labels):\n",
        "  # train the model for 10 epochs, with 32 datapoints per batch.\n",
        "  # 10 epochs is very short, for demonstration purposes. In a real setting we'd train for much longer.\n",
        "  model.fit(training_data, labels, epochs=10, batch_size=32, verbose=1)\n",
        "  return model"
      ],
      "metadata": {
        "id": "mLtIQ4KMdr_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we do a bit of preprocessing, split the data into sets for training and for testing, then make and train the model using the make_model() and train() functions we defined above. This will take approximately 30 seconds. Usually, we'd train for much longer than we do here.\n",
        "\n",
        "*Train a model called first_model:*"
      ],
      "metadata": {
        "id": "ObeRPZIEhIqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(['TRANSIENT'], axis=1)  # X is our training data, without the labels\n",
        "Y = data['TRANSIENT']  # Y is our labels\n",
        "# normalize all the data to fall within the range of 0 and 1\n",
        "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "scaler.fit(X.to_numpy())\n",
        "X_scaled = scaler.transform(X.to_numpy())\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X_scaled, Y.to_numpy())  # split the data into training and testing sets, with sensor data and labels seperate\n",
        "\n",
        "first_model = make_model()  # define first_model\n",
        "first_model = train(first_model, xtrain, ytrain)  # train first_model"
      ],
      "metadata": {
        "id": "yEYCoBHSeVMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Evaluate the model's accuracy on the test data:*"
      ],
      "metadata": {
        "id": "pmbsNIUEheId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = first_model.evaluate(xtest, ytest)\n",
        "accuracy = (results[1]) * 100\n",
        "print(f'\\n YOU (naively): \\n \"This model is awesome! It correctly classifies {accuracy:.0f}% of the test data. I\\'m ready to publish this!\"')"
      ],
      "metadata": {
        "id": "9rqpOZweesyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, the model might appear ready to use in real-world contexts. Imagine that you've been given new data from a client or user.\n",
        "\n",
        "*Import the new data, and look it over:*"
      ],
      "metadata": {
        "id": "tTnPuMUYhkx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv(\"https://raw.githubusercontent.com/DunyaBahar/CyberNuke/main/new_data.csv\")\n",
        "new_data.head()"
      ],
      "metadata": {
        "id": "wXRCuVgqfc5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Compare that to the first few rows of the old data:*"
      ],
      "metadata": {
        "id": "iEe-UfBkLbC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(xtest).head()"
      ],
      "metadata": {
        "id": "ph4n5hhOT4OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a makeshift form of security, you may want to compare the new data to the data you had previously, which you know to be clean and untampered."
      ],
      "metadata": {
        "id": "TwxexM8CLi2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Challenge 1:\n",
        "Check for any glaring discrepancies between the two sets of data (new_data above and xtest below) which could indicate that the new data has been tampered with.\n",
        "\n",
        "- Are there any non-integer values in the new data which were binary values (0 or 1) in the clean data, or vice versa? For features which we know to be categorcial, we expect values of 0 or 1, and no non-integer values.\n",
        "- Do any values in the new data fall below 0 or over 1? Since we normalized our data, we expect that the new data will also stay within this range.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "M9WwNWcCh2I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "write your response to challenge 1 here...\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "y_k1AYQ4LWaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've inspected the data and are confident it's clean, you might tell the client that they can expect the model's performance on the new data to about what it was during your testing (perhaps 80-90% accuracy).\n",
        "\n",
        "Imagine the client comes back to you a month later, demanding a refund and complaining that the model has dismally low accuracy. They painstakingly hand-labeled the first 1000 datapoints for you, and ask you to check the model's predictions against those true labels.\n",
        "\n",
        "*Evaluate the model on the client's data:*"
      ],
      "metadata": {
        "id": "2uzb1zP_iIGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_results = first_model.evaluate(new_data[:1000], ytest[:1000])\n",
        "new_results = (new_results[1]) * 100\n",
        "\n",
        "print(f'\\n YOU (suprised): \\n \"You\\'re right, the model only correctly classified {new_results:.0f}% of your data. I think the model is being attacked!\"')"
      ],
      "metadata": {
        "id": "jkeFQQrHRUvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Projected Gradient Descent Attack\n",
        "Adversarial attacks involve creating carefully-crafted \"adversarial examples\" or tampered datapoints. When fed though a model, adversarial examples are misclassified at a high rate, tanking the model's accuracy.\n",
        "\n",
        " The Projected Gradient Descent (PGD) attack is a white-box adversarial attack. The term \"white-box\" means that the attacker would need access to the model to implement the attack.\n",
        "\n",
        " The attack is an iterative variant of the Fast Gradient Sign Method (FGSM) attack, with an added step of projection. The PGD attack generates adversarial examples by maximizing the loss between the target label and the model's predicted labels, with a contraint that no single feature can be perturbed beyond some small threshold which we'll call epsilon. The attack is able to create large missclassifications and dramatically reduce the accuracy of the model, with just small and often imperceptible changes to the input data."
      ],
      "metadata": {
        "id": "_pQsQVwB4wZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Adversarial Training Defense\n",
        "\n",
        "Adversarial training is a proactive defense strategy used to combat adversarial attacks like the PGD attack. The idea is to generate lots of adversarial examples, or perturbed data, and include that in the training set for your model.\n",
        "\n",
        "When the model is trained on many adversarial examples, it's able to learn how to correctly classify them in the future! This is a preventative defense- time and effort is invested up front with the goal of making the model more robust. The adversarial training should make your model less vulnerable to potential future attacks.\n",
        "\n",
        "To perform adversarial training, you first need to think like an attacker. We'll implement a Projected Gradient Descent attack, so that we can include the generated adversarial data in our training set.\n",
        "\n",
        "*Generate 5 adversarial examples based on the first 5 datapoints from our clean xtest dataset:*"
      ],
      "metadata": {
        "id": "o0a90DHUUFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_perturb = xtest[:5]  # this is the clean data that we will use as a starting point for our adversarial examples.\n",
        "data_to_perturb = tf.convert_to_tensor(data_to_perturb, dtype=tf.float32)\n",
        "pd.DataFrame(data_to_perturb)  # show the clean data which we will perturb"
      ],
      "metadata": {
        "id": "_Im5ayl3qUMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's allow any feature value for a given data point to be perturbed up to 5%. We will assume that this amount of change in the datapoint's feature values doesn't affect the true label of that datapoint.\n",
        "\n",
        "For example, if you know that 2x + 3y - 10z = A, we could increase or decrease the coefficients on x, y, and z up to 5% of their original value, and the function would still be equal to A.\n",
        "\n",
        "So, we know that:\n",
        "2.1x + 3.15y - 9.5z still equals A (increasing each coefficient by 5%).\n",
        "1.9x + 2.85y - 10.5z still equals A (decreasing each coefficient by 5%).\n",
        "2.05x + 2.9y - 10z still equals A (increasing or decreasing each coefficient by no more than 5%).\n",
        "And so on.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6QhpN9Nrh6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Challenge 2:\n",
        "Whether this assumption is actually true matters for the stealthiness and overall efficacy of the attack. Imagine you were working with image data. If you were allowed to change the color of each pixel by 5%, would the resulting image always maintain the original image's label? For example, if you were given an image of a tree, are you sure that no pertubations within the 5% limit would make the image appear to be something else entirely, like a car or a panda? How could you test these assumptions to settle on an ideal and set epsilon value for a specific use-case?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "c6jvnnksNHaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "write your response to challenge 2 here...\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "laUsKn7gK_Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 27 features in this dataset, so we're looking at 27-dimensional space. Lets pretend we only have 2 so that we can visualize the algorithm in a comprehendible way. Say we have only the \"CALCULATED AVERAGE TEMPERATURE\" and the \"PRESSURIZER PRESSURE\" features, and our labels in the \"TRANSIENTS\" category. Imagine the temperature on the x axis, the pressure on the y axis, and the labels on the z axis. If the temperature were 517 and the pressure were 2218, we know our label will be \"Transient Valve Closure\", which we've assigned to the numerical value 3.\n",
        "\n",
        "With an epsilon value of .05 (the 5% we discussed above), we can form a square centered around the point (517, 2218, 3). We'll project our purtubations onto the perimeter of this square, hence the \"projected\" in the attack name of \"projected gradient descent\".\n",
        "\n",
        "With 3 features, we project the pertubations onto the surface of a cube rather than a square. In real-world high-dimensional data, we project them onto a hyper-cube."
      ],
      "metadata": {
        "id": "WQee3tdjK2-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Initialize the pertubations to random points within the hypercube surrounding each clean datapoint, and define the loss object as keras's sparse categorical cross-entropy function:*"
      ],
      "metadata": {
        "id": "OI9aQotE2rqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = .05\n",
        "perturbations = tf.random.uniform(shape=data_to_perturb.shape, minval=-eps, maxval=eps)\n",
        "perturbations = tf.convert_to_tensor(perturbations, dtype=tf.float32)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
      ],
      "metadata": {
        "id": "1JaBXY6crgw8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*While tracking the gradient of the loss function with respect to the pertubations, create adversarial data by adding the pertubations to the clean data:*"
      ],
      "metadata": {
        "id": "ACYBuitu3J3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will want to perform this block of times over multiple iterations to maximize the efficacy of our adversarial example.\n",
        "\n",
        "labels = ytest[:5]\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(perturbations)\n",
        "    adversarial_data = data_to_perturb + perturbations  # add the pertubation to the clean data to form the first attaempt at an adversarial datapoint\n",
        "    predictions = first_model(adversarial_data)  # see what the model predicts to be the label of the new datapoint\n",
        "    loss = loss_object(labels, predictions)  # understand how close our datapoint's label was to the target label"
      ],
      "metadata": {
        "id": "skmMR81crsre"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Determine the sign of the gradient of the loss function with respect to the pertubations, and update the pertubations by a small step in that direction:*"
      ],
      "metadata": {
        "id": "Wg6GYoXMOgFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = .0005  # in most PGD attack implementations, this value is much smaller than epsilon.\n",
        "gradient = tape.gradient(loss, perturbations)  # the slope of the loss function hyperplane\n",
        "normalized_gradient = tf.sign(gradient)  # The sign of the gradient (positivie or negative, +1 or -1) is enough information, we don't need the actual magnitude of the gradient.\n",
        "perturbations += step_size * normalized_gradient  # set the pertubation equal to a step size in the direction of the gradient\n",
        "adversarial_examples = data_to_perturb + perturbations  # set our adversarial example to be the clean datapoint but a step uphill or downhill on ___?"
      ],
      "metadata": {
        "id": "r23hR0CjrsoG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Repeat the above steps over several iterations:*"
      ],
      "metadata": {
        "id": "agvUUmxt5_xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iter = 50\n",
        "for i in range(num_iter):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(perturbations)\n",
        "    adversarial_data = data_to_perturb + perturbations\n",
        "    prediction = first_model(adversarial_data)\n",
        "    loss = loss_object(labels, prediction)\n",
        "  gradient = tape.gradient(loss, perturbations)\n",
        "  normalized_gradient = tf.sign(gradient)\n",
        "  perturbations += step_size * normalized_gradient\n",
        "  adversarial_examples = data_to_perturb + perturbations\n",
        "  adversarial_examples = np.clip(adversarial_examples, -eps, eps)"
      ],
      "metadata": {
        "id": "9RYzdFD85_F6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Examine the 5 adversarial examples we've just created:*"
      ],
      "metadata": {
        "id": "jLa08SnW6XsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(adversarial_examples)"
      ],
      "metadata": {
        "id": "qSJjYyPC6cwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Show the original 5 clean datapoints, for comparison:*"
      ],
      "metadata": {
        "id": "ey3pQKpfPztd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(xtest[:5])"
      ],
      "metadata": {
        "id": "5Zwny85PPa7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When visually comparing these adversarial examples to the original clean ones, we can see that our data have negative values, wheras the clean data appear to have been normalized to fall within the range of 0 and 1. Successful attack examples should not differ in obvious ways from clean data, or else the attack will not be discreet. We want to mimic a realistic attack, so that our adversarial training data will transfer as much robustness as possible to a real-world attack."
      ],
      "metadata": {
        "id": "AXbQEcqXO81J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Fix the discrepancy by normalizing the perturbed data to fall within the range of 0 and 1, as the clean data does:*"
      ],
      "metadata": {
        "id": "yVKprzxQPX2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial_examples = np.clip(adversarial_examples, 0, 1)  # values less than 0 will become 0, and values greater than 1 will become 1.\n",
        "pd.DataFrame(adversarial_examples)"
      ],
      "metadata": {
        "id": "GBFewtFO6g43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's still one obvious difference between our attack data and the clean data: the clean data has some binary features, and our adversarial examples aren't being limited to 0s and 1s like we'd expect them to be. This could make our attack more detectable, since we would know that non-binary data doesn't belong in those columns.\n",
        "\n",
        "*Fix the discrepancy by reverting the values of categorical features back to what they were in the associated clean datapoint:*"
      ],
      "metadata": {
        "id": "qm45SJqKPhQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine which columns hold only binary data in the clean dataset\n",
        "categorical_features = []\n",
        "for i in range(xtest.shape[1]):\n",
        "    unique_values = np.unique(xtest[:, i])\n",
        "    if len(unique_values) == 2 and np.isclose(unique_values, [0, 1]).all():  # chek whether the data is only 0s and 1s\n",
        "        categorical_features.append(i)\n",
        "# revert those values back to what they were in the clean data\n",
        "adversarial_examples[:, categorical_features] = xtest[:5, categorical_features]\n"
      ],
      "metadata": {
        "id": "shfy4JoyPVzx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Check that no obvious indications of tampering with the data remain:*"
      ],
      "metadata": {
        "id": "ka6OAqB0RMPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(adversarial_examples)"
      ],
      "metadata": {
        "id": "gvif74byRN_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine the PGD attack code into one function, and then use it to generate a larger adversarial dataset. Then, we evaluate our model on the adversarial data to ensure that our attack is effective.\n",
        "\n",
        "*Condense the attack code into a single function:*"
      ],
      "metadata": {
        "id": "afZHf2lYaLrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Explanation of function parameters:\n",
        "----------------------------------\n",
        "- clean_data: This is the original, unperturbed data that you are trying to generate adversarial examples with.\n",
        "- target: These are the target labels for the clean data.\n",
        "- model: This is the naive neural network model that you are attacking.\n",
        "- eps: Short for epsilon. This controls the max change allowed per feature (L-infinity norm constraint).\n",
        "- step_size: This is the step size in the gradient descent/ascent process.\n",
        "- num_iter: The number of iterations to perform. This controls how many steps are taken in the gradient descent process.\n",
        "'''\n",
        "\n",
        "def pgd(clean_data, target, model, eps=.05, step_size=.0005, num_iter=100):\n",
        "    categorical_features = []\n",
        "    for i in range(clean_data.shape[1]):\n",
        "        unique_values = np.unique(clean_data[:, i])\n",
        "        if len(unique_values) == 2 and np.isclose(unique_values, [0, 1]).all():\n",
        "            categorical_features.append(i)\n",
        "    perturbations = tf.random.uniform(shape=clean_data.shape, minval=-eps, maxval=eps)\n",
        "    perturbations = tf.convert_to_tensor(perturbations, dtype=tf.float32)\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    for i in range(num_iter):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(perturbations)\n",
        "            adversarial_data = clean_data + perturbations\n",
        "            prediction = model(adversarial_data)\n",
        "            loss = loss_object(target, prediction)\n",
        "        gradient = tape.gradient(loss, perturbations)\n",
        "        normalized_gradient = tf.sign(gradient)\n",
        "        perturbations += step_size * normalized_gradient\n",
        "        adversarial_examples = clean_data + perturbations\n",
        "        adversarial_examples = np.clip(adversarial_examples, 0, 1)\n",
        "        adversarial_examples[:, categorical_features] = clean_data[:, categorical_features]\n",
        "    return adversarial_examples"
      ],
      "metadata": {
        "id": "l5opnZWFiwK3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Use the above pgd( ) function to generate 1000 adversarial examples from xtest:*"
      ],
      "metadata": {
        "id": "61dyh_S-R_J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pgd_data = pgd(xtest[:1000], ytest[:1000], first_model)"
      ],
      "metadata": {
        "id": "zoXfcdBzQblQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Check that the attack effectively lowered the model's accuracy:*"
      ],
      "metadata": {
        "id": "x6y-5xEqSNv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attack_results = first_model.evaluate(pgd_data, ytest[:1000])[1] * 100\n",
        "clean_results = first_model.evaluate(xtest[:1000], ytest[:1000])[1] * 100\n",
        "\n",
        "print(f'\\nModel accuracy on clean data: {clean_results}')\n",
        "print(f'Model accuracy on attack data: {attack_results}')\n",
        "print(f'\\nAttack successful! We brought the model\\'s accuracy down by { clean_results - attack_results:.0f} percentage points!')"
      ],
      "metadata": {
        "id": "WP3N1mX6QkZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the attack was effective, it would be worthwhile to include that data in the training data of the model. By seeing the adversarial data in advance, the model will be more prepared for similar attacks in the future.\n",
        "\n",
        "# Next Steps for Adversarial Training:\n",
        "- Use the above pgd( ) function to generate an adversarial example for each clean example in the training set, to form a 2nd adversarial dataset.\n",
        "- Train a second model (with the same architecture as the first model) on a combined dataset consisting of both the clean and adversarial examples.\n",
        "- Generate a 3rd set of PGD examples using xtest and the new robust model.\n",
        "- Test the robust model's accuracy on that 3rd set, and compare the improved accuracy against the naive model's accuracy during the first PGD attack.\n",
        "\n",
        "Adversarial training is a promising and relatively effective defense against adversarial attacks. By including a variety of attack types in your training data, the model's new robustness will transfer to a wider variety of attacks. Adversarial typically dramatically increases robustness against adversarial attacks, while slightly decreasing the model's accuracy on clean data.\n",
        "\n",
        "Great work!"
      ],
      "metadata": {
        "id": "JmVQfnzdSZG2"
      }
    }
  ]
}